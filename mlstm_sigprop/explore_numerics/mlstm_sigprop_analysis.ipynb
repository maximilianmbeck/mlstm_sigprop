{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(linewidth=400, threshold=100000)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from mlstm_kernels.components.ln import MultiHeadLayerNorm\n",
    "from mlstm_kernels.mlstm.chunkwise.max_triton_fwbw_v3 import (\n",
    "    mlstm_chunkwise_max_triton_v3,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive plot of the mLSTM Signal Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.bfloat16  # torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: mLSTM implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlstm_paper_unstable_fgate(\n",
    "    matQ: torch.Tensor,\n",
    "    matK: torch.Tensor,\n",
    "    matV: torch.Tensor,\n",
    "    vecI: torch.Tensor,\n",
    "    vecF: torch.Tensor,\n",
    "    eps: float = 1e-6,\n",
    "    mstate_mode: str = \"paper\",\n",
    ") -> torch.Tensor:\n",
    "    import math\n",
    "\n",
    "    B, NH, S, DHQK = matQ.shape\n",
    "    assert matK.shape == (B, NH, S, DHQK)\n",
    "    assert vecI.shape == (B, NH, S)\n",
    "    assert vecF.shape == (B, NH, S)\n",
    "\n",
    "    _dtype, _device = matQ.dtype, matQ.device\n",
    "\n",
    "    vecLogSigF = F.logsigmoid(vecF)  # (B, NH, S)\n",
    "    vecLogSigF_cumsum = vecLogSigF.cumsum(-1)\n",
    "\n",
    "    matLogSigF = vecLogSigF_cumsum[:, :, :, None] - vecLogSigF_cumsum[:, :, None, :]\n",
    "\n",
    "    ltr = torch.tril(\n",
    "        torch.ones(\n",
    "            (S, S),\n",
    "            dtype=torch.bool,\n",
    "            device=_device,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    matLogSigF_mask = torch.where(ltr, matLogSigF, -float(\"inf\"))\n",
    "\n",
    "    matLogD = matLogSigF_mask + vecI[:, :, None, :]\n",
    "\n",
    "    vecM, _ = torch.max(matLogD, dim=-1, keepdim=True)  # (B, NH, S, 1)\n",
    "    matLogD_stabilized = matLogD - vecM\n",
    "\n",
    "    matD = torch.exp(matLogD_stabilized)  # (B, NH, S, S)\n",
    "\n",
    "    matS = (matQ @ matK.transpose(-2, -1)) / math.sqrt(DHQK)  # (B, NH, S, S)\n",
    "\n",
    "    matCtilde = matS * matD  # (B, NH, S, S)\n",
    "    if mstate_mode == \"paper\":\n",
    "        vecN = torch.maximum(matCtilde.sum(dim=-1, keepdim=True).abs(), torch.exp(-vecM))  # (B, NH, S, 1)\n",
    "    elif mstate_mode == \"exp_minus_m_to_one\":\n",
    "        vecN = torch.maximum(\n",
    "            matCtilde.sum(dim=-1, keepdim=True).abs(),\n",
    "            torch.tensor([1.0], device=_device, dtype=_dtype),\n",
    "        )  # (B, NH, S, 1)\n",
    "    elif mstate_mode == \"sum_only\":\n",
    "        vecN = matCtilde.sum(dim=-1, keepdim=True).abs()\n",
    "\n",
    "    elif mstate_mode == \"denom_one\":\n",
    "        vecN = torch.tensor([1.0], device=_device, dtype=_dtype)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"mstate_mode {mstate_mode} not recognized\")\n",
    "\n",
    "    # (B, NH, S, S)\n",
    "    matC = matCtilde / (vecN + eps)\n",
    "\n",
    "    matH = matC @ matV  # (B, NH, S, DH)\n",
    "\n",
    "    return (\n",
    "        matH,\n",
    "        vecM.squeeze(-1),\n",
    "        vecN.squeeze(-1),\n",
    "        matLogD,\n",
    "        matLogD_stabilized,\n",
    "        matD,\n",
    "        matCtilde,\n",
    "        matC,\n",
    "        vecLogSigF,\n",
    "        vecLogSigF_cumsum,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlstm_paper_stable_fgate(\n",
    "    matQ: torch.Tensor,\n",
    "    matK: torch.Tensor,\n",
    "    matV: torch.Tensor,\n",
    "    vecI: torch.Tensor,\n",
    "    vecF: torch.Tensor,\n",
    "    eps: float = 1e-6,\n",
    "    mstate_mode: str = \"paper\",\n",
    ") -> torch.Tensor:\n",
    "    import math\n",
    "\n",
    "    B, NH, S, DHQK = matQ.shape\n",
    "    assert matK.shape == (B, NH, S, DHQK)\n",
    "    assert vecI.shape == (B, NH, S)\n",
    "    assert vecF.shape == (B, NH, S)\n",
    "\n",
    "    _dtype, _device = matQ.dtype, matQ.device\n",
    "\n",
    "    vecLogSigF = F.logsigmoid(vecF)  # (B, NH, S)\n",
    "\n",
    "    matLogSigF_tril = vecLogSigF[:, :, :, None].repeat(1, 1, 1, S).tril(-1)\n",
    "    matLogSigF_cum = matLogSigF_tril.cumsum(-2)\n",
    "\n",
    "    ltr = torch.tril(\n",
    "        torch.ones(\n",
    "            (S, S),\n",
    "            dtype=torch.bool,\n",
    "            device=_device,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    matLogSigF_mask = torch.where(ltr, matLogSigF_cum, -float(\"inf\"))\n",
    "\n",
    "    matLogD = matLogSigF_mask + vecI[:, :, None, :]\n",
    "\n",
    "    vecM, _ = torch.max(matLogD, dim=-1, keepdim=True)  # (B, NH, S, 1)\n",
    "    matLogD_stabilized = matLogD - vecM\n",
    "\n",
    "    matD = torch.exp(matLogD_stabilized)  # (B, NH, S, S)\n",
    "\n",
    "    matS = (matQ @ matK.transpose(-2, -1)) / math.sqrt(DHQK)  # (B, NH, S, S)\n",
    "\n",
    "    matCtilde = matS * matD  # (B, NH, S, S)\n",
    "    if mstate_mode == \"paper\":\n",
    "        vecN = torch.maximum(matCtilde.sum(dim=-1, keepdim=True).abs(), torch.exp(-vecM))  # (B, NH, S, 1)\n",
    "    elif mstate_mode == \"exp_minus_m_to_one\":\n",
    "        vecN = torch.maximum(\n",
    "            matCtilde.sum(dim=-1, keepdim=True).abs(),\n",
    "            torch.tensor([1.0], device=_device, dtype=_dtype),\n",
    "        )  # (B, NH, S, 1)\n",
    "    elif mstate_mode == \"sum_only\":\n",
    "        vecN = matCtilde.sum(dim=-1, keepdim=True).abs()\n",
    "\n",
    "    elif mstate_mode == \"denom_one\":\n",
    "        vecN = torch.tensor([1.0], device=_device, dtype=_dtype)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"mstate_mode {mstate_mode} not recognized\")\n",
    "    # (B, NH, S, S)\n",
    "    matC = matCtilde / (vecN + eps)\n",
    "\n",
    "    matH = matC @ matV  # (B, NH, S, DH)\n",
    "\n",
    "    return (\n",
    "        matH,\n",
    "        vecM.squeeze(-1),\n",
    "        vecN.squeeze(-1),\n",
    "        matLogD,\n",
    "        matLogD_stabilized,\n",
    "        matD,\n",
    "        matCtilde,\n",
    "        matC,\n",
    "        vecLogSigF,\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: Plotting code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matplotlib plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func,\n",
    "    B,\n",
    "    NH,\n",
    "    S,\n",
    "    DHQK,\n",
    "    DHV,\n",
    "    vecI_offset,\n",
    "    vecF_offset,\n",
    "    seed=0,\n",
    "    plot_max_min=True,\n",
    "    vecI_init_fn=torch.randn,\n",
    "    vecF_init_fn=torch.randn,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    matQ = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matK = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matV = torch.randn((B, NH, S, DHV), dtype=DTYPE, device=DEVICE)\n",
    "    # vecI = 0.00001 * torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "    # vecF = -30. + torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "    vecI = vecI_offset + vecI_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "    vecF = vecF_offset + vecF_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "    out = mlstm_func(matQ, matK, matV, vecI, vecF)\n",
    "\n",
    "    if isinstance(out, tuple):\n",
    "        (\n",
    "            h_out,\n",
    "            m_out,\n",
    "            n_out,\n",
    "            matLogD,\n",
    "            matLogD_stabilized,\n",
    "            matD,\n",
    "            matCtilde,\n",
    "            matC,\n",
    "            vecLogSigF,\n",
    "            vecLogSigF_cumsum,\n",
    "        ) = out\n",
    "    else:\n",
    "        h_out = out\n",
    "        m_out = None\n",
    "        n_out = None\n",
    "        matLogD = None\n",
    "        matLogD_stabilized = None\n",
    "        matD = None\n",
    "        matCtilde = None\n",
    "        matC = None\n",
    "        vecLogSigF = None\n",
    "        vecLogSigF_cumsum = None\n",
    "\n",
    "    # plot hout + mstate\n",
    "    h_out_pl_mean = h_out.mean(-1).flatten().cpu().float().numpy()\n",
    "    h_out_pl_std = h_out.std(-1).flatten().cpu().float().numpy()\n",
    "    h_out_max = h_out.max(-1)[0].flatten().cpu().float().numpy()\n",
    "    h_out_min = h_out.min(-1)[0].flatten().cpu().float().numpy()\n",
    "    if m_out is not None:\n",
    "        m_pl = m_out.flatten().cpu().float().numpy()\n",
    "        plt.plot(m_pl, label=\"m_state\")\n",
    "    # plt.plot(f_pl, label=\"f_preact\")\n",
    "    # plt.plot(flogsig_pl)\n",
    "    # plt.plot(n_pl, label=\"n_state\")\n",
    "    plt.plot(h_out_pl_mean, label=\"h_out_mean\")\n",
    "    plt.fill_between(\n",
    "        range(len(h_out_pl_mean)),\n",
    "        h_out_pl_mean - h_out_pl_std,\n",
    "        h_out_pl_mean + h_out_pl_std,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    if plot_max_min:\n",
    "        plt.plot(h_out_max, label=\"h_out_max\")\n",
    "        plt.plot(h_out_min, label=\"h_out_min\")\n",
    "\n",
    "    plt.legend()\n",
    "    print(f\"vecI_offs: {vecI_offset}, vecF_offs: {vecF_offset}\")\n",
    "    print(f\"S: {S}, B: {B}, NH: {NH}, DHQK: {DHQK}, DHV: {DHV}\")\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "    return (\n",
    "        h_out,\n",
    "        m_out,\n",
    "        n_out,\n",
    "        matLogD,\n",
    "        matLogD_stabilized,\n",
    "        matD,\n",
    "        matCtilde,\n",
    "        matC,\n",
    "        vecLogSigF,\n",
    "        vecLogSigF_cumsum,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set vecF_offset to -6 (almost any negative value) produces an m_state spike\n",
    "B = 1\n",
    "NH = 1\n",
    "S = 8192\n",
    "D = 1024\n",
    "DHQK = D\n",
    "DHV = D\n",
    "vecI_offset = -3.0  # -3.0\n",
    "vecF_offset = 5.0\n",
    "vecI_init_fn = torch.randn\n",
    "vecF_init_fn = torch.randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"paper version\")\n",
    "(\n",
    "    h_out,\n",
    "    m_out,\n",
    "    n_out,\n",
    "    matLogD,\n",
    "    matLogD_stabilized,\n",
    "    matD,\n",
    "    matCtilde,\n",
    "    matC,\n",
    "    vecLogSigF,\n",
    "    vecLogSigF_cumsum,\n",
    ") = make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func=partial(mlstm_paper_unstable_fgate, mstate_mode=\"paper\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")\n",
    "print(\"exp_minus_m_to_one\")\n",
    "_ = make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func=partial(mlstm_paper_unstable_fgate, mstate_mode=\"exp_minus_m_to_one\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")\n",
    "print(\"denom_one\")\n",
    "(\n",
    "    h_out_do,\n",
    "    m_out_do,\n",
    "    n_out_do,\n",
    "    matLogD_do,\n",
    "    matLogD_stabilized_do,\n",
    "    matD_do,\n",
    "    matCtilde_do,\n",
    "    matC_do,\n",
    "    vecLogSigF_do,\n",
    "    vecLogSigF_cumsum_do,\n",
    ") = make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func=partial(mlstm_paper_unstable_fgate, mstate_mode=\"denom_one\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")\n",
    "print(\"max_triton_v3 kernel\")\n",
    "_ = make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func=mlstm_chunkwise_max_triton_v3,\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")\n",
    "print(\"==== stable torch version ====\")\n",
    "print(\"paper version\")\n",
    "(\n",
    "    h_out,\n",
    "    m_out,\n",
    "    n_out,\n",
    "    matLogD,\n",
    "    matLogD_stabilized,\n",
    "    matD,\n",
    "    matCtilde,\n",
    "    matC,\n",
    "    vecLogSigF,\n",
    "    vecLogSigF_cumsum,\n",
    ") = make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func=partial(mlstm_paper_stable_fgate, mstate_mode=\"paper\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")\n",
    "print(\"exp_minus_m_to_one\")\n",
    "_ = make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func=partial(mlstm_paper_stable_fgate, mstate_mode=\"exp_minus_m_to_one\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")\n",
    "print(\"denom_one\")\n",
    "(\n",
    "    h_out_do,\n",
    "    m_out_do,\n",
    "    n_out_do,\n",
    "    matLogD_do,\n",
    "    matLogD_stabilized_do,\n",
    "    matD_do,\n",
    "    matCtilde_do,\n",
    "    matC_do,\n",
    "    vecLogSigF_do,\n",
    "    vecLogSigF_cumsum_do,\n",
    ") = make_h_output_plot_mlstm_with_internals(\n",
    "    mlstm_func=partial(mlstm_paper_stable_fgate, mstate_mode=\"denom_one\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### static plotly plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_h_output_plot_mlstm_with_internals_plotly(\n",
    "    mlstm_func,\n",
    "    B,\n",
    "    NH,\n",
    "    S,\n",
    "    DHQK,\n",
    "    DHV,\n",
    "    vecI_offset,\n",
    "    vecF_offset,\n",
    "    seed=0,\n",
    "    plot_max_min=True,\n",
    "    vecI_init_fn=torch.randn,\n",
    "    vecF_init_fn=torch.randn,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    matQ = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matK = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matV = torch.randn((B, NH, S, DHV), dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "    vecI = vecI_offset + vecI_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "    vecF = vecF_offset + vecF_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "    out = mlstm_func(matQ, matK, matV, vecI, vecF)\n",
    "\n",
    "    if isinstance(out, tuple):\n",
    "        (\n",
    "            h_out,\n",
    "            m_out,\n",
    "            n_out,\n",
    "            matLogD,\n",
    "            matLogD_stabilized,\n",
    "            matD,\n",
    "            matCtilde,\n",
    "            matC,\n",
    "            vecLogSigF,\n",
    "            vecLogSigF_cumsum,\n",
    "        ) = out\n",
    "    else:\n",
    "        h_out = out\n",
    "        m_out = None\n",
    "        n_out = None\n",
    "        matLogD = None\n",
    "        matLogD_stabilized = None\n",
    "        matD = None\n",
    "        matCtilde = None\n",
    "        matC = None\n",
    "        vecLogSigF = None\n",
    "        vecLogSigF_cumsum = None\n",
    "\n",
    "    # Data preparation\n",
    "    h_out_pl_mean = h_out.mean(-1).flatten().cpu().float().numpy()\n",
    "    h_out_pl_std = h_out.std(-1).flatten().cpu().float().numpy()\n",
    "    h_out_max = h_out.max(-1)[0].flatten().cpu().float().numpy()\n",
    "    h_out_min = h_out.min(-1)[0].flatten().cpu().float().numpy()\n",
    "\n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot m_out if it exists\n",
    "    if m_out is not None:\n",
    "        m_pl = m_out.flatten().cpu().float().numpy()\n",
    "        fig.add_trace(go.Scatter(y=m_pl, mode=\"lines\", name=\"m_state\"))\n",
    "\n",
    "    # Plot h_out_mean\n",
    "    fig.add_trace(go.Scatter(y=h_out_pl_mean, mode=\"lines\", name=\"h_out_mean\"))\n",
    "\n",
    "    # Add shaded region for standard deviation\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(h_out_pl_mean))) + list(range(len(h_out_pl_mean))[::-1]),\n",
    "            y=list(h_out_pl_mean + h_out_pl_std) + list((h_out_pl_mean - h_out_pl_std)[::-1]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(0,100,80,0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            showlegend=False,\n",
    "            name=\"h_out_std\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot max and min if required\n",
    "    if plot_max_min:\n",
    "        fig.add_trace(go.Scatter(y=h_out_max, mode=\"lines\", name=\"h_out_max\"))\n",
    "        fig.add_trace(go.Scatter(y=h_out_min, mode=\"lines\", name=\"h_out_min\"))\n",
    "\n",
    "    # Update figure layout\n",
    "    fig.update_layout(\n",
    "        title=\"h_out and Internal States\",\n",
    "        xaxis_title=\"Sequence Position\",\n",
    "        yaxis_title=\"Value\",\n",
    "        legend_title=\"Legend\",\n",
    "    )\n",
    "\n",
    "    print(f\"vecI_offs: {vecI_offset}, vecF_offs: {vecF_offset}\")\n",
    "    print(f\"S: {S}, B: {B}, NH: {NH}, DHQK: {DHQK}, DHV: {DHV}\")\n",
    "    # Display the plot\n",
    "    fig.show()\n",
    "\n",
    "    return (\n",
    "        h_out,\n",
    "        m_out,\n",
    "        n_out,\n",
    "        matLogD,\n",
    "        matLogD_stabilized,\n",
    "        matD,\n",
    "        matCtilde,\n",
    "        matC,\n",
    "        vecLogSigF,\n",
    "        vecLogSigF_cumsum,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"paper version\")\n",
    "(\n",
    "    h_out,\n",
    "    m_out,\n",
    "    n_out,\n",
    "    matLogD,\n",
    "    matLogD_stabilized,\n",
    "    matD,\n",
    "    matCtilde,\n",
    "    matC,\n",
    "    vecLogSigF,\n",
    "    vecLogSigF_cumsum,\n",
    ") = make_h_output_plot_mlstm_with_internals_plotly(\n",
    "    mlstm_func=partial(mlstm_paper_unstable_fgate, mstate_mode=\"paper\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset=vecI_offset,\n",
    "    vecF_offset=vecF_offset,\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variable plotly plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_input_mean_std_max_min(matQKV):\n",
    "    q_pl_mean = matQKV.mean(-1).flatten().cpu().float().numpy()\n",
    "    q_pl_std = matQKV.std(-1).flatten().cpu().float().numpy()\n",
    "    q_pl_max = matQKV.max(-1)[0].flatten().cpu().float().numpy()\n",
    "    q_pl_min = matQKV.min(-1)[0].flatten().cpu().float().numpy()\n",
    "    plt.plot(q_pl_mean, label=\"qkv_mean\")\n",
    "    plt.fill_between(\n",
    "        range(len(q_pl_mean)),\n",
    "        q_pl_mean - q_pl_std,\n",
    "        q_pl_mean + q_pl_std,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.plot(q_pl_max, label=\"qkv_max\")\n",
    "    plt.plot(q_pl_min, label=\"qkv_min\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_h_output_plot_mlstm_with_internals_with_separate_sliders(\n",
    "    mlstm_func,\n",
    "    B,\n",
    "    NH,\n",
    "    S,\n",
    "    DHQK,\n",
    "    DHV,\n",
    "    vecI_offset_range,  # Tuple of (min, max, step) for vecI_offset\n",
    "    vecF_offset_range,  # Tuple of (min, max, step) for vecF_offset\n",
    "    seed=0,\n",
    "    plot_max_min=True,\n",
    "    plot_m_state=True,\n",
    "    vecI_init_fn=torch.randn,\n",
    "    vecF_init_fn=torch.randn,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    matQ = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matK = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matV = torch.randn((B, NH, S, DHV), dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "    plot_input_mean_std_max_min(matQ)\n",
    "\n",
    "    # Function to generate the plot data based on vecI_offset and vecF_offset\n",
    "    def get_plot_data(vecI_offset, vecF_offset):\n",
    "        vecI = vecI_offset + vecI_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "        vecF = vecF_offset + vecF_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "        out = mlstm_func(matQ, matK, matV, vecI, vecF)\n",
    "\n",
    "        if isinstance(out, tuple):\n",
    "            h_out = out[0]\n",
    "            m_out = out[1]\n",
    "        else:\n",
    "            h_out = out\n",
    "            m_out = None\n",
    "\n",
    "        h_out_pl_mean = h_out.mean(-1).flatten().cpu().float().numpy()\n",
    "        h_out_pl_std = h_out.std(-1).flatten().cpu().float().numpy()\n",
    "        h_out_max = h_out.max(-1)[0].flatten().cpu().float().numpy()\n",
    "        h_out_min = h_out.min(-1)[0].flatten().cpu().float().numpy()\n",
    "        if m_out is not None:\n",
    "            m_pl = m_out.flatten().cpu().float().numpy()\n",
    "        else:\n",
    "            m_pl = None\n",
    "\n",
    "        if not plot_m_state:\n",
    "            m_pl = None\n",
    "\n",
    "        return h_out_pl_mean, h_out_pl_std, h_out_max, h_out_min, m_pl\n",
    "\n",
    "    # Initial plot data\n",
    "    if isinstance(vecI_offset_range, tuple):\n",
    "        vecI_min, vecI_max, vecI_step = vecI_offset_range\n",
    "        vecI_values = torch.arange(vecI_min, vecI_max + vecI_step, vecI_step)\n",
    "    elif isinstance(vecI_offset_range, list):\n",
    "        vecI_values = torch.tensor(vecI_offset_range)\n",
    "    else:\n",
    "        raise ValueError(\"vecI_offset_range must be a tuple or list\")\n",
    "\n",
    "    if isinstance(vecF_offset_range, tuple):\n",
    "        vecF_min, vecF_max, vecF_step = vecF_offset_range\n",
    "        vecF_values = torch.arange(vecF_min, vecF_max + vecF_step, vecF_step)\n",
    "    elif isinstance(vecF_offset_range, list):\n",
    "        vecF_values = torch.tensor(vecF_offset_range)\n",
    "    else:\n",
    "        raise ValueError(\"vecF_offset_range must be a tuple or list\")\n",
    "\n",
    "    # Store data for combinations of vecI_offset and vecF_offset\n",
    "    data_cache = {}\n",
    "    for vecI_offset in vecI_values:\n",
    "        for vecF_offset in vecF_values:\n",
    "            data_cache[(vecI_offset.item(), vecF_offset.item())] = get_plot_data(vecI_offset, vecF_offset)\n",
    "\n",
    "    # Initial values\n",
    "    initial_vecI = vecI_values[0].item()\n",
    "    initial_vecF = vecF_values[-1].item()\n",
    "    h_out_pl_mean, h_out_pl_std, h_out_max, h_out_min, m_pl = data_cache[(initial_vecI, initial_vecF)]\n",
    "\n",
    "    # Create figure and add initial traces\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=h_out_pl_mean, mode=\"lines\", name=\"h_out_mean\"))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(h_out_pl_mean))) + list(range(len(h_out_pl_mean))[::-1]),\n",
    "            y=list(h_out_pl_mean + h_out_pl_std) + list((h_out_pl_mean - h_out_pl_std)[::-1]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(0,100,80,0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            showlegend=False,\n",
    "            name=\"h_out_std\",\n",
    "        )\n",
    "    )\n",
    "    if plot_max_min:\n",
    "        fig.add_trace(go.Scatter(y=h_out_max, mode=\"lines\", name=\"h_out_max\"))\n",
    "        fig.add_trace(go.Scatter(y=h_out_min, mode=\"lines\", name=\"h_out_min\"))\n",
    "\n",
    "    fig.add_trace(go.Scatter(y=m_pl, mode=\"lines\", name=\"m_state\"))\n",
    "\n",
    "    # Create frames for each combination of vecI_offset and vecF_offset\n",
    "    frames = []\n",
    "    for vecI_offset in vecI_values:\n",
    "        for vecF_offset in vecF_values:\n",
    "            h_out_pl_mean, h_out_pl_std, h_out_max, h_out_min, m_pl = data_cache[\n",
    "                (vecI_offset.item(), vecF_offset.item())\n",
    "            ]\n",
    "            frames.append(\n",
    "                go.Frame(\n",
    "                    data=[\n",
    "                        go.Scatter(y=m_pl, mode=\"lines\", name=\"m_state\"),\n",
    "                        go.Scatter(y=h_out_pl_mean, mode=\"lines\", name=f\"h_out_mean\"),\n",
    "                        go.Scatter(\n",
    "                            x=list(range(len(h_out_pl_mean))) + list(range(len(h_out_pl_mean))[::-1]),\n",
    "                            y=list(h_out_pl_mean + h_out_pl_std) + list((h_out_pl_mean - h_out_pl_std)[::-1]),\n",
    "                            fill=\"toself\",\n",
    "                            fillcolor=\"rgba(0,100,80,0.2)\",\n",
    "                            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "                            showlegend=True,\n",
    "                            name=\"h_out_std\",\n",
    "                        ),\n",
    "                        go.Scatter(y=h_out_max, mode=\"lines\", name=\"h_out_max\"),\n",
    "                        go.Scatter(y=h_out_min, mode=\"lines\", name=\"h_out_min\"),\n",
    "                    ],\n",
    "                    name=f\"{vecI_offset.item()}_{vecF_offset.item()}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Add frames to the figure\n",
    "    fig.frames = frames\n",
    "\n",
    "    # Create sliders for vecI_offset and vecF_offset\n",
    "    #! TODO max this does not work as they depend on each other\n",
    "    slider_vecI = {\n",
    "        \"currentvalue\": {\"prefix\": \"vecI_offset: \"},\n",
    "        \"pad\": {\"t\": 50},\n",
    "        \"steps\": [\n",
    "            {\n",
    "                \"args\": [\n",
    "                    [f\"{vecI_offset.item()}_{initial_vecF}\"],\n",
    "                    {\"frame\": {\"duration\": 0, \"redraw\": True}, \"mode\": \"immediate\"},\n",
    "                ],\n",
    "                \"label\": f\"{vecI_offset.item()}\",\n",
    "                \"method\": \"animate\",\n",
    "            }\n",
    "            for vecI_offset in vecI_values\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    slider_vecF = {\n",
    "        \"currentvalue\": {\"prefix\": \"vecF_offset: \"},\n",
    "        \"pad\": {\"t\": 140},\n",
    "        \"steps\": [\n",
    "            {\n",
    "                \"args\": [\n",
    "                    [f\"{initial_vecI}_{vecF_offset.item()}\"],\n",
    "                    {\"frame\": {\"duration\": 0, \"redraw\": True}, \"mode\": \"immediate\"},\n",
    "                ],\n",
    "                \"label\": f\"{vecF_offset.item()}\",\n",
    "                \"method\": \"animate\",\n",
    "            }\n",
    "            for vecF_offset in vecF_values\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    #! workaround with just one slider\n",
    "    slider_vecIF = {\n",
    "        \"currentvalue\": {\"prefix\": \"vecIF_offset: \"},\n",
    "        \"pad\": {\"t\": 50},\n",
    "        \"steps\": [\n",
    "            {\n",
    "                \"args\": [\n",
    "                    [f\"{vecI_offset.item()}_{vecF_offset.item()}\"],\n",
    "                    {\"frame\": {\"duration\": 0, \"redraw\": True}, \"mode\": \"immediate\"},\n",
    "                ],\n",
    "                \"label\": f\"I_off={vecI_offset.item()}, F_off={vecF_offset.item()}\",\n",
    "                \"method\": \"animate\",\n",
    "            }\n",
    "            for vecI_offset in vecI_values\n",
    "            for vecF_offset in vecF_values\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Update layout with two sliders\n",
    "    fig.update_layout(\n",
    "        sliders=[slider_vecIF],  # [slider_vecI, slider_vecF],\n",
    "        title=\"Signal propagation in mLSTM\",\n",
    "        xaxis_title=\"Sequence Position\",\n",
    "        yaxis_title=\"Value\",\n",
    "        legend_title=\"Legend\",\n",
    "        width=1000,\n",
    "        height=1200,\n",
    "    )\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Status quo of input + forget gate with different offsets\n",
    "\n",
    "Goal: we want to have roughly the same max/min and mean + std values of the random input. then we consider the mlstm as stable. (of course it still should do some information routing)\n",
    "\n",
    "Conclusion: \n",
    "- the input gate preact needs a (larger) negative bias in order to avoid the feature spikes (large max/min values)\n",
    "    - in between (-3, -6)\n",
    "- forget gate preact needs larger positive bias (sigmoid(large x) = 1) in order to keep the max/min values on the qkv max/min level\n",
    "    - around 6 ca.\n",
    "\n",
    "- these values indicate that the input and forget gates are not independent of each other:\n",
    "    - high fgate offset means much of the input is kept, i.e. the \"sum window\" is larger hence larger max min values\n",
    "    - in order to avoid that the max / min values explode one can now also decrease the input gate by making the offset more negative, then many smaller values will be summed\n",
    "    - how can we solve that?\n",
    "\n",
    "- bfloat16 artefacts:\n",
    "    - we see some numerical artefacts with the unstabilized fgate version that happen when the fgate offset is <-3 (m_state spikes)\n",
    "    - we do not see this in the torch stabilized version and also NOT in the max_triton_v3 kernel!\n",
    "\n",
    "Issue Summary: \n",
    "1. Coupling of input and forget gate. \n",
    "2. fgate offset sensitivity\n",
    "3. igate offset sensitivity\n",
    "\n",
    "Fixes for 1.\n",
    "- Just make both learnable and hope that gradient decent finds an equilibrium\n",
    "    - maybe by addressing 2. and 3., learning this gets easier\n",
    "- look at trained models and adapt initialization\n",
    "    - could also help the first \"hope\"\n",
    "\n",
    "Fixes for 2.\n",
    "- make the fgate less dependent on the bias -> use the GLA / Mamba trick\n",
    "- add an additional non learnable bias (offset in code)\n",
    "- adapt initialization\n",
    "\n",
    "Fixes for 3.\n",
    "- additional non learnable bias (offset in code)\n",
    "- adapt initialization\n",
    "    - first experiment on this is very promising at 7B scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"paper version unstable fgate\")\n",
    "\n",
    "fig = make_h_output_plot_mlstm_with_internals_with_separate_sliders(\n",
    "    mlstm_func=partial(mlstm_paper_unstable_fgate, mstate_mode=\"exp_minus_m_to_one\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=[-6, -3, 0],\n",
    "    vecF_offset_range=(-6, 10, 1),\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"paper version stable fgate\")\n",
    "\n",
    "fig = make_h_output_plot_mlstm_with_internals_with_separate_sliders(\n",
    "    mlstm_func=partial(mlstm_paper_stable_fgate, mstate_mode=\"paper\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=[-6, -3, 0],\n",
    "    vecF_offset_range=(-6, 10, 1),\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"paper version max_triton_v3 kernels\")\n",
    "\n",
    "fig = make_h_output_plot_mlstm_with_internals_with_separate_sliders(\n",
    "    mlstm_func=mlstm_chunkwise_max_triton_v3,\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=[-6, -3, 0],\n",
    "    vecF_offset_range=(-6, 10, 1),\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix attempt \"glafgate\" for (2) fgate sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlstm_stable_fgate_gla(\n",
    "    matQ: torch.Tensor,\n",
    "    matK: torch.Tensor,\n",
    "    matV: torch.Tensor,\n",
    "    vecI: torch.Tensor,\n",
    "    vecF: torch.Tensor,\n",
    "    tau: float = 1.0,\n",
    "    eps: float = 1e-6,\n",
    "    mstate_mode: str = \"paper\",\n",
    ") -> torch.Tensor:\n",
    "    import math\n",
    "\n",
    "    B, NH, S, DHQK = matQ.shape\n",
    "    assert matK.shape == (B, NH, S, DHQK)\n",
    "    assert vecI.shape == (B, NH, S)\n",
    "    assert vecF.shape == (B, NH, S)\n",
    "\n",
    "    _dtype, _device = matQ.dtype, matQ.device\n",
    "\n",
    "    vecLogSigF = torch.log(torch.sigmoid(vecF) ** (1 / tau))  # (B, NH, S)\n",
    "\n",
    "    matLogSigF_tril = vecLogSigF[:, :, :, None].repeat(1, 1, 1, S).tril(-1)\n",
    "    matLogSigF_cum = matLogSigF_tril.cumsum(-2)\n",
    "\n",
    "    ltr = torch.tril(\n",
    "        torch.ones(\n",
    "            (S, S),\n",
    "            dtype=torch.bool,\n",
    "            device=_device,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    matLogSigF_mask = torch.where(ltr, matLogSigF_cum, -float(\"inf\"))\n",
    "\n",
    "    matLogD = matLogSigF_mask + vecI[:, :, None, :]\n",
    "\n",
    "    vecM, _ = torch.max(matLogD, dim=-1, keepdim=True)  # (B, NH, S, 1)\n",
    "    matLogD_stabilized = matLogD - vecM\n",
    "\n",
    "    matD = torch.exp(matLogD_stabilized)  # (B, NH, S, S)\n",
    "\n",
    "    matS = (matQ @ matK.transpose(-2, -1)) / math.sqrt(DHQK)  # (B, NH, S, S)\n",
    "\n",
    "    matCtilde = matS * matD  # (B, NH, S, S)\n",
    "    if mstate_mode == \"paper\":\n",
    "        vecN = torch.maximum(matCtilde.sum(dim=-1, keepdim=True).abs(), torch.exp(-vecM))  # (B, NH, S, 1)\n",
    "    elif mstate_mode == \"exp_minus_m_to_one\":\n",
    "        vecN = torch.maximum(\n",
    "            matCtilde.sum(dim=-1, keepdim=True).abs(),\n",
    "            torch.tensor([1.0], device=_device, dtype=_dtype),\n",
    "        )  # (B, NH, S, 1)\n",
    "    elif mstate_mode == \"sum_only\":\n",
    "        vecN = matCtilde.sum(dim=-1, keepdim=True).abs()\n",
    "\n",
    "    elif mstate_mode == \"denom_one\":\n",
    "        vecN = torch.tensor([1.0], device=_device, dtype=_dtype)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"mstate_mode {mstate_mode} not recognized\")\n",
    "    # (B, NH, S, S)\n",
    "    matC = matCtilde / (vecN + eps)\n",
    "\n",
    "    matH = matC @ matV  # (B, NH, S, DH)\n",
    "\n",
    "    return (\n",
    "        matH,\n",
    "        vecM.squeeze(-1),\n",
    "        vecN.squeeze(-1),\n",
    "        matLogD,\n",
    "        matLogD_stabilized,\n",
    "        matD,\n",
    "        matCtilde,\n",
    "        matC,\n",
    "        vecLogSigF,\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fix attempt GLA fgate version stable fgate\")\n",
    "\n",
    "fig = make_h_output_plot_mlstm_with_internals_with_separate_sliders(\n",
    "    mlstm_func=partial(mlstm_stable_fgate_gla, mstate_mode=\"paper\", tau=1.0),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=[-6, -3, 0],\n",
    "    vecF_offset_range=(-6, 10, 1),\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fix attempt GLA fgate version stable fgate\")\n",
    "\n",
    "fig = make_h_output_plot_mlstm_with_internals_with_separate_sliders(\n",
    "    mlstm_func=partial(mlstm_stable_fgate_gla, mstate_mode=\"paper\", tau=16.0),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=[-6, -3, 0],\n",
    "    vecF_offset_range=(-6, 10, 1),\n",
    "    seed=0,\n",
    "    vecI_init_fn=vecI_init_fn,\n",
    "    vecF_init_fn=vecF_init_fn,\n",
    "    plot_m_state=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Sweep over vecI, vecF offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mlstm_outputs(\n",
    "    mlstm_func,\n",
    "    B,\n",
    "    NH,\n",
    "    S,\n",
    "    DHQK,\n",
    "    DHV,\n",
    "    vecI_offset,\n",
    "    vecF_offset,\n",
    "    vecI_init_fn=torch.randn,\n",
    "    vecF_init_fn=torch.randn,\n",
    "):\n",
    "    torch.manual_seed(0)\n",
    "    matQ = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matK = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "    matV = torch.randn((B, NH, S, DHV), dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "    vecI = vecI_offset + vecI_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "    vecF = vecF_offset + vecF_init_fn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "    out = mlstm_func(matQ, matK, matV, vecI, vecF)\n",
    "\n",
    "    if isinstance(out, tuple):\n",
    "        (\n",
    "            h_out,\n",
    "            m_out,\n",
    "            n_out,\n",
    "            matLogD,\n",
    "            matLogD_stabilized,\n",
    "            matD,\n",
    "            matCtilde,\n",
    "            matC,\n",
    "            vecLogSigF,\n",
    "            vecLogSigF_cumsum,\n",
    "        ) = out\n",
    "    else:\n",
    "        h_out = out\n",
    "        m_out = None\n",
    "        n_out = None\n",
    "        matLogD = None\n",
    "        matLogD_stabilized = None\n",
    "        matD = None\n",
    "        matCtilde = None\n",
    "        matC = None\n",
    "        vecLogSigF = None\n",
    "        vecLogSigF_cumsum = None\n",
    "\n",
    "    return (\n",
    "        h_out,\n",
    "        m_out,\n",
    "        n_out,\n",
    "        matLogD,\n",
    "        matLogD_stabilized,\n",
    "        matD,\n",
    "        matCtilde,\n",
    "        matC,\n",
    "        vecLogSigF,\n",
    "        vecLogSigF_cumsum,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_offset_sweep(\n",
    "    mlstm_func,\n",
    "    B,\n",
    "    NH,\n",
    "    S,\n",
    "    DHQK,\n",
    "    DHV,\n",
    "    vecI_offset_range,\n",
    "    vecF_offset_range,\n",
    "    vecI_init_fn=torch.randn,\n",
    "    vecF_init_fn=torch.randn,\n",
    "    metric: str = \"h_out_max_mean\",\n",
    "):\n",
    "    data = []\n",
    "    data_tensor = torch.zeros(len(vecI_offset_range), len(vecF_offset_range))\n",
    "    for i, vecI_offset in enumerate(vecI_offset_range):\n",
    "        for j, vecF_offset in enumerate(vecF_offset_range):\n",
    "            out = compute_mlstm_outputs(\n",
    "                mlstm_func,\n",
    "                B,\n",
    "                NH,\n",
    "                S,\n",
    "                DHQK,\n",
    "                DHV,\n",
    "                vecI_offset,\n",
    "                vecF_offset,\n",
    "                vecI_init_fn,\n",
    "                vecF_init_fn,\n",
    "            )\n",
    "            if metric == \"h_out_max_mean\":\n",
    "                h_out = out[0]\n",
    "                h_out_max = h_out.max(-1)[0].mean()\n",
    "                metric_val = h_out_max\n",
    "            else:\n",
    "                raise ValueError(f\"metric {metric} not recognized\")\n",
    "            data_val = {\n",
    "                \"vecI_offset\": vecI_offset.item(),\n",
    "                \"vecF_offset\": vecF_offset.item(),\n",
    "                \"metric\": metric_val.item(),\n",
    "            }\n",
    "            data.append(data_val)\n",
    "            data_tensor[i, j] = metric_val.cpu()\n",
    "\n",
    "    return data, data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set vecF_offset to -6 (almost any negative value) produces an m_state spike\n",
    "B = 1\n",
    "NH = 1\n",
    "S = 2048\n",
    "D = 1024\n",
    "DHQK = D\n",
    "DHV = D\n",
    "vecI_offset = -3.0  # -3.0\n",
    "vecF_offset = 5.0\n",
    "vecI_init_fn = torch.randn\n",
    "vecF_init_fn = torch.randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecI_offset_range = torch.linspace(-8, 6, 50)\n",
    "vecF_offset_range = torch.linspace(-5, 12, 50)\n",
    "data, data_tensor = make_offset_sweep(\n",
    "    mlstm_func=partial(mlstm_paper_unstable_fgate, mstate_mode=\"paper\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=vecI_offset_range,\n",
    "    vecF_offset_range=vecF_offset_range,\n",
    "    metric=\"h_out_max_mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "grid_x, grid_y = torch.meshgrid(vecF_offset_range, vecI_offset_range, indexing=\"ij\")\n",
    "grid_x = grid_x.cpu().numpy()\n",
    "grid_y = grid_y.cpu().numpy()\n",
    "data_z = data_tensor.transpose(0, 1).cpu().numpy()\n",
    "\n",
    "# levels = mpl.ticker.MaxNLocator(nbins=20).tick_values(data_z.min(), data_z.max())\n",
    "levels = np.linspace(0, 10, 10)\n",
    "cmap = plt.colormaps[\"PiYG\"]\n",
    "norm = mpl.colors.BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
    "\n",
    "im = ax.pcolormesh(grid_x, grid_y, data_z, cmap=cmap, norm=norm)\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_title(label=\"h_out max (over feature dim) (mean over time)\")\n",
    "ax.set_ylabel(\"vecI_offset\")\n",
    "ax.set_xlabel(\"vecF_offset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecI_offset_range = torch.linspace(-8, 6, 50)\n",
    "vecF_offset_range = torch.linspace(-5, 12, 50)\n",
    "data, data_tensor = make_offset_sweep(\n",
    "    mlstm_func=partial(mlstm_paper_unstable_fgate, mstate_mode=\"paper\"),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=vecI_offset_range,\n",
    "    vecF_offset_range=vecF_offset_range,\n",
    "    metric=\"h_out_max_mean\",\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "grid_x, grid_y = torch.meshgrid(vecF_offset_range, vecI_offset_range, indexing=\"ij\")\n",
    "grid_x = grid_x.cpu().numpy()\n",
    "grid_y = grid_y.cpu().numpy()\n",
    "data_z = data_tensor.transpose(0, 1).cpu().numpy()\n",
    "\n",
    "# levels = mpl.ticker.MaxNLocator(nbins=20).tick_values(data_z.min(), data_z.max())\n",
    "levels = np.linspace(0, 10, 10)\n",
    "cmap = plt.colormaps[\"PiYG\"]\n",
    "norm = mpl.colors.BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
    "\n",
    "im = ax.pcolormesh(grid_x, grid_y, data_z, cmap=cmap, norm=norm)\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_title(label=\"h_out max (over feature dim) (mean over time)\")\n",
    "ax.set_ylabel(\"vecI_offset\")\n",
    "ax.set_xlabel(\"vecF_offset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecI_offset_range = torch.linspace(-8, 6, 50)\n",
    "vecF_offset_range = torch.linspace(-5, 12, 50)\n",
    "data, data_tensor = make_offset_sweep(\n",
    "    mlstm_func=partial(mlstm_stable_fgate_gla, mstate_mode=\"paper\", tau=16.0),\n",
    "    B=B,\n",
    "    NH=NH,\n",
    "    S=S,\n",
    "    DHQK=DHQK,\n",
    "    DHV=DHV,\n",
    "    vecI_offset_range=vecI_offset_range,\n",
    "    vecF_offset_range=vecF_offset_range,\n",
    "    metric=\"h_out_max_mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "grid_x, grid_y = torch.meshgrid(vecF_offset_range, vecI_offset_range, indexing=\"ij\")\n",
    "grid_x = grid_x.cpu().numpy()\n",
    "grid_y = grid_y.cpu().numpy()\n",
    "data_z = data_tensor.transpose(0, 1).cpu().numpy()\n",
    "\n",
    "# levels = mpl.ticker.MaxNLocator(nbins=20).tick_values(data_z.min(), data_z.max())\n",
    "levels = np.linspace(0, 10, 10)\n",
    "cmap = plt.colormaps[\"PiYG\"]\n",
    "norm = mpl.colors.BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
    "\n",
    "im = ax.pcolormesh(grid_x, grid_y, data_z, cmap=cmap, norm=norm)\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_title(label=\"h_out max (over feature dim) (mean over time)\")\n",
    "ax.set_ylabel(\"vecI_offset\")\n",
    "ax.set_xlabel(\"vecF_offset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_idx in range(len(vecI_offset_range)):\n",
    "    plt.plot(\n",
    "        vecF_offset_range.cpu().numpy(),\n",
    "        data_tensor[i_idx, :].cpu().numpy(),\n",
    "        label=f\"vecI_offset={vecI_offset_range[i_idx].item()}\",\n",
    "    )\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt240cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
